{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpXFq9nkK4bX",
        "outputId": "18fdb080-b86f-41c7-b31c-e4a1db5a15c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAumjtyjJyRA",
        "outputId": "d5e157d7-736b-4e7e-838c-1905486a7e7e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m835.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -U -qq git+https://github.com/huggingface/diffusers.git\n",
        "!pip install -qq accelerate tensorboard transformers ftfy gradio\n",
        "!pip install -qq \"ipywidgets>=7,<8\"\n",
        "!pip install -qq bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5xZVqyWJyRI",
        "outputId": "4a6752ed-270b-4c1d-8b47-ebbfd6a660a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Your GPU may not be supported for xformers precompiled wheels.\n"
          ]
        }
      ],
      "source": [
        "!pip install -U --pre triton\n",
        "\n",
        "from subprocess import getoutput\n",
        "import time\n",
        "\n",
        "s = getoutput('nvidia-smi')\n",
        "if 'T4' in s:\n",
        "    gpu = 'T4'\n",
        "elif 'P100' in s:\n",
        "    gpu = 'P100'\n",
        "elif 'V100' in s:\n",
        "    gpu = 'V100'\n",
        "elif 'A100' in s:\n",
        "    gpu = 'A100'\n",
        "else:\n",
        "    gpu = None\n",
        "    print(\"Your GPU may not be supported for xformers precompiled wheels.\")\n",
        "\n",
        "if gpu == 'T4':\n",
        "    %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl\n",
        "elif gpu == 'P100':\n",
        "    %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/P100/xformers-0.0.13.dev0-py3-none-any.whl\n",
        "elif gpu == 'V100':\n",
        "    %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/V100/xformers-0.0.13.dev0-py3-none-any.whl\n",
        "elif gpu == 'A100':\n",
        "    %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/xformers-0.0.13.dev0-py3-none-any.whl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O384EDXpJyRM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "38f18353-94bd-485c-a6fe-b4b55963fcef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:bitsandbytes.cextension:Could not load bitsandbytes native library: /lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.32' not found (required by /usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cpu.so)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/cextension.py\", line 85, in <module>\n",
            "    lib = get_native_library()\n",
            "          ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/cextension.py\", line 72, in get_native_library\n",
            "    dll = ct.cdll.LoadLibrary(str(binary_path))\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ctypes/__init__.py\", line 454, in LoadLibrary\n",
            "    return self._dlltype(name)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ctypes/__init__.py\", line 376, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "OSError: /lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.32' not found (required by /usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cpu.so)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import itertools\n",
        "import gc\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from accelerate import Accelerator, notebook_launcher, logging as accel_logging, utils as accel_utils\n",
        "from accelerate.utils import set_seed\n",
        "\n",
        "from diffusers import (\n",
        "    AutoencoderKL,\n",
        "    DDPMScheduler,\n",
        "    StableDiffusionPipeline,\n",
        "    UNet2DConditionModel,\n",
        ")\n",
        "from diffusers.optimization import get_scheduler\n",
        "\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "import bitsandbytes as bnb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ShG97mkNMEO",
        "outputId": "be1f596d-059b-40e1-c256-acd427416b5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMG_20230423_082747.jpg    IMG_20240528_105934.jpg  IMG_20240903_062903.jpg\n",
            "IMG_20230423_083415.jpg    IMG_20240531_014610.jpg  IMG_20240911_063611.jpg\n",
            "IMG_20230901_095400.jpg    IMG_20240712_092230.jpg  IMG_20240911_063615.jpg\n",
            "IMG_20230901_095403.jpg    IMG_20240712_092238.jpg  IMG_20240911_063636.jpg\n",
            "IMG_20231101_172932.jpg    IMG_20240712_092244.jpg  IMG_20240911_063732.jpg\n",
            "IMG_20240303_070457~3.jpg  IMG_20240825_113037.jpg  IMG_20240911_063840.jpg\n",
            "IMG_20240308_072035.jpg    IMG_20240901_163621.jpg\n",
            "IMG_20240528_105928.jpg    IMG_20240901_163641.jpg\n"
          ]
        }
      ],
      "source": [
        "!dir \"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeKTbhKIJyRO"
      },
      "outputs": [],
      "source": [
        "# The pretrained Stable Diffusion model to start from\n",
        "pretrained_model_name_or_path = r\"/content/drive/MyDrive/CS614 Final Project/model/\"\n",
        "\n",
        "# Unique prompt identifier for your pet dog Bruno\n",
        "instance_prompt = \"<bruno> a photo of my pet dog Bruno\"\n",
        "\n",
        "# Path to your custom dataset of Bruno's images (ensure this folder exists)\n",
        "instance_data_dir = r\"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan/\"\n",
        "\n",
        "# Option for prior preservation (set to False if you only train on Bruno)\n",
        "with_prior_preservation = False\n",
        "prior_loss_weight = 1.0  # adjust if using prior preservation\n",
        "\n",
        "# If using prior preservation, you can specify a class prompt and folder for generic dog images:\n",
        "class_prompt = \"a photo of a dog\"\n",
        "class_data_dir = r\"/content/drive/MyDrive/CS614 Final Project/class_images\"  # make sure this folder exists (or let the code create it)\n",
        "num_class_images = 12  # number of class images to generate/use\n",
        "\n",
        "# Training hyperparameters\n",
        "learning_rate = 5e-6\n",
        "max_train_steps = 300  # adjust based on dataset size and experimentation\n",
        "train_batch_size = 2\n",
        "gradient_accumulation_steps = 2\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "# Mixed precision and gradient checkpointing settings\n",
        "mixed_precision = \"fp16\"  # or \"bf16\" if supported\n",
        "gradient_checkpointing = True\n",
        "use_8bit_adam = True\n",
        "\n",
        "# Output directory to save checkpoints and the final model\n",
        "output_dir = \"dreambooth-bruno\"\n",
        "seed = 3434554\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPQDlxJ4Th5I"
      },
      "outputs": [],
      "source": [
        "# Captions for images in your dataset\n",
        "captions = {\n",
        "    r\"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan/IMG_20231101_172932.jpg\": \"<bruno> sitting happily with his tongue out, looking cheerful and excited. The bright expression on his face and his slightly tilted head give him an adorable and friendly vibe.\",\n",
        "\n",
        "    r\"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan/IMG_20230423_082747.jpg\": \"<bruno> lying comfortably on a green platform, looking up with his tongue out in a joyful and relaxed manner. His soft golden fur contrasts beautifully with the background.\",\n",
        "\n",
        "    r\"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan/IMG_20230423_083415.jpg\": \"<bruno> sitting inside his playpen, full of energy and excitement. His bright eyes and wagging tail suggest he is happy and eager for attention.\",\n",
        "\n",
        "    r\"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan/IMG_20230901_095400.jpg\": \"<bruno> lying on the ground with a big happy grin, enjoying the moment as he gets a gentle pat on his chin. His playful and affectionate nature shines through in this close-up shot.\",\n",
        "\n",
        "    r\"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan/IMG_20230901_095403.jpg\": \"<bruno> looking up with excitement and love, his mouth open in a playful and joyful expression. His ears perked up show he is attentive and enjoying the interaction.\",\n",
        "\n",
        "    r\"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan/IMG_20240531_014610.jpg\": \"<bruno> lying on the floor in a relaxed state, gazing calmly at the camera. His peaceful expression and stretched-out posture suggest he's winding down after a long day.\",\n",
        "\n",
        "    r\"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan/IMG_20240303_070457~3.jpg\": \"<bruno> enjoying the outdoors, lying on patterned pavement with a bright expression. His tongue hanging out and the silver chain around his neck add to his playful and confident look.\",\n",
        "\n",
        "    r\"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan/IMG_20240308_072035.jpg\": \"<bruno> standing with his front paws on the bed, eagerly looking at the camera with his tongue out. His eyes are filled with excitement, and the bookshelf in the background adds a cozy indoor setting.\",\n",
        "\n",
        "    r\"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan/IMG_20240528_105928.jpg\": \"<bruno> sitting on a rough outdoor surface, looking up curiously with his mouth slightly open. His slightly dusty fur and bright eyes show that he's been having an adventurous time outside.\",\n",
        "\n",
        "    r\"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan/IMG_20240528_105934.jpg\": \"<bruno> sitting attentively, gazing upwards with an excited expression. His playful energy is evident as he tilts his head slightly, his tongue peeking out while he enjoys the outdoor breeze.\",\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhUjZTQ9Thyx"
      },
      "outputs": [],
      "source": [
        "captions.update({\n",
        "    r\"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan/IMG_20240712_092230.jpg\": \"<bruno> lounging on a beautifully tiled outdoor patio, basking in the warm sunlight. His tongue is out in a happy and relaxed expression, while the greenery and house in the background add to the cozy, homey vibe.\",\n",
        "})\n",
        "\n",
        "captions.update({\n",
        "    r\"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan/IMG_20240712_092238.jpg\": \"<bruno> lying on a patterned outdoor tile floor, looking cheerful with his tongue out. The bright morning sunlight and lush greenery in the background create a perfect setting for his playful and content expression.\",\n",
        "})\n",
        "\n",
        "\n",
        "captions.update({\n",
        "    r\"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan/IMG_20240712_092244.jpg\": \"<bruno> happily lounging on the outdoor patio, his tongue hanging out as he enjoys the fresh air. The vibrant greenery and sunlit background create a perfect contrast to his relaxed yet playful posture.\",\n",
        "})\n",
        "\n",
        "captions.update({\n",
        "    r\"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan/IMG_20240825_113037.jpg\": \"<bruno> resting on a patterned tiled floor near a bench, looking relaxed yet alert. His tongue is out, showing his happy and content mood, while the decorative window grille in the background adds a unique architectural touch to the scene.\",\n",
        "})\n",
        "\n",
        "captions.update({\n",
        "    r\"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan/IMG_20240901_163621.jpg\": \"<bruno> lying on a cool marble floor, happily panting with his tongue out. His bright eyes and playful expression make it clear he's enjoying a relaxed moment indoors, with a blue cupboard in the background adding contrast to his light fur.\",\n",
        "})\n",
        "\n",
        "captions.update({\n",
        "    r\"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan/IMG_20240901_163641.jpg\": \"<bruno> lying comfortably on the cool marble floor, his tongue out as he pants happily. The bright lighting highlights his soft golden fur, and the deep blue cupboard in the background adds a striking contrast to his relaxed yet playful mood.\",\n",
        "})\n",
        "\n",
        "captions.update({\n",
        "    r\"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan/IMG_20240903_062903.jpg\": \"<bruno> lying down on the damp outdoor pavement, his eyes filled with sadness and exhaustion. His head rests heavily on the ground, and the misty morning setting, moss-covered walls, and distant houses add to the melancholic and lonely atmosphere of the scene.\",\n",
        "})\n",
        "\n",
        "captions.update({\n",
        "    r\"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan/IMG_20240911_063611.jpg\": \"<bruno> sitting on a patterned outdoor pavement, looking up with an innocent and slightly curious expression. His mouth is slightly open as if he’s mid-whimper or about to bark, and the lush greenery with blooming flowers in the background adds a vibrant touch to the peaceful morning scene.\",\n",
        "})\n",
        "\n",
        "captions.update({\n",
        "    r\"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan/IMG_20240911_063615.jpg\": \"<bruno> sitting attentively on the outdoor pavement, gazing up with a curious yet serious expression. His slightly raised eyebrows and closed mouth give him a thoughtful look, while the lush green plants and vibrant flowers in the background add to the peaceful morning ambiance.\",\n",
        "})\n",
        "\n",
        "captions.update({\n",
        "    r\"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan/IMG_20240911_063636.jpg\": \"<bruno> sitting upright on the outdoor pavement, gazing upwards with a thoughtful and slightly longing expression. His soft eyes and the way his ears gently droop give him an endearing, almost wistful look, while the lush green plants and red flower pots add to the serene morning atmosphere.\",\n",
        "})\n",
        "\n",
        "captions.update({\n",
        "    r\"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan/IMG_20240911_063732.jpg\": \"<bruno> lying gracefully on the outdoor pavement, gazing into the distance with a calm and regal expression. His strong posture, paired with the lush greenery and blooming flowers in the background, gives him a noble and watchful presence in the peaceful morning setting.\",\n",
        "})\n",
        "\n",
        "captions.update({\n",
        "    r\"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan/IMG_20240911_063840.jpg\": \"<bruno> resting his head on the cool pavement, gazing into the distance with deep, soulful eyes. His expression carries a quiet sadness or longing, as if lost in thought. The lush green plants and red flower pots in the background add to the serene and contemplative mood of the image.\",\n",
        "})\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWNozXTfT69L",
        "outputId": "1b85d007-0e00-40f7-ffe2-1c5fca797573"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "len(captions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f0IJBI5JyRP"
      },
      "outputs": [],
      "source": [
        "from argparse import Namespace\n",
        "\n",
        "args = Namespace(\n",
        "    pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
        "    resolution=512,  # Stable Diffusion typically uses 512x512 images\n",
        "    center_crop=True,\n",
        "    train_text_encoder=False,\n",
        "    instance_data_dir=instance_data_dir,\n",
        "    instance_prompt=instance_prompt,\n",
        "    learning_rate=learning_rate,\n",
        "    max_train_steps=max_train_steps,\n",
        "    save_steps=50,\n",
        "    train_batch_size=train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    mixed_precision=mixed_precision,\n",
        "    gradient_checkpointing=gradient_checkpointing,\n",
        "    use_8bit_adam=use_8bit_adam,\n",
        "    seed=seed,\n",
        "    with_prior_preservation=with_prior_preservation,\n",
        "    prior_loss_weight=prior_loss_weight,\n",
        "    sample_batch_size=train_batch_size,  # For generating class images if needed\n",
        "    class_data_dir=class_data_dir,\n",
        "    class_prompt=class_prompt,\n",
        "    num_class_images=num_class_images,\n",
        "    lr_scheduler=\"constant\",\n",
        "    lr_warmup_steps=100,\n",
        "    output_dir=output_dir,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQhuaZsyJyRQ"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "class DreamBoothDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        instance_data_root,\n",
        "        instance_prompts,  # This can be a dict mapping filename -> caption or a list of captions.\n",
        "        tokenizer,\n",
        "        class_data_root=None,\n",
        "        class_prompt=None,\n",
        "        size=512,\n",
        "        center_crop=False,\n",
        "    ):\n",
        "        self.size = size\n",
        "        self.center_crop = center_crop\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.instance_data_root = Path(instance_data_root)\n",
        "        if not self.instance_data_root.exists():\n",
        "            raise ValueError(\"Instance images root doesn't exist.\")\n",
        "\n",
        "        # List of image paths sorted alphabetically for consistency\n",
        "        self.instance_images_path = sorted(list(self.instance_data_root.iterdir()))\n",
        "        self.num_instance_images = len(self.instance_images_path)\n",
        "\n",
        "        # instance_prompts can be a dict or a list. If not, we assume it's a single prompt.\n",
        "        if isinstance(instance_prompts, dict):\n",
        "            self.captions_dict = instance_prompts\n",
        "        elif isinstance(instance_prompts, list):\n",
        "            if len(instance_prompts) != self.num_instance_images:\n",
        "                raise ValueError(\"Length of prompts list must match number of images.\")\n",
        "            self.captions_list = instance_prompts\n",
        "        else:\n",
        "            self.default_prompt = instance_prompts  # use the same prompt for every image\n",
        "\n",
        "        self._length = self.num_instance_images\n",
        "\n",
        "        # Setup class images if prior preservation is enabled\n",
        "        if class_data_root is not None:\n",
        "            self.class_data_root = Path(class_data_root)\n",
        "            self.class_data_root.mkdir(parents=True, exist_ok=True)\n",
        "            self.class_images_path = list(self.class_data_root.iterdir())\n",
        "            self.num_class_images = len(self.class_images_path)\n",
        "            self._length = max(self.num_instance_images, self.num_class_images)\n",
        "            self.class_prompt = class_prompt\n",
        "        else:\n",
        "            self.class_data_root = None\n",
        "\n",
        "        self.image_transforms = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.5], [0.5]),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = {}\n",
        "        # Get the image path\n",
        "        image_path = self.instance_images_path[index % self.num_instance_images]\n",
        "        instance_image = Image.open(image_path)\n",
        "        if instance_image.mode != \"RGB\":\n",
        "            instance_image = instance_image.convert(\"RGB\")\n",
        "        example[\"instance_images\"] = self.image_transforms(instance_image)\n",
        "\n",
        "        # Determine the caption for this image\n",
        "        if hasattr(self, \"captions_dict\"):\n",
        "            # Use the filename to lookup the caption\n",
        "            caption = self.captions_dict.get(image_path.name, self.default_prompt if hasattr(self, \"default_prompt\") else \"\")\n",
        "        elif hasattr(self, \"captions_list\"):\n",
        "            caption = self.captions_list[index % self.num_instance_images]\n",
        "        else:\n",
        "            caption = self.default_prompt\n",
        "\n",
        "        # Tokenize the caption\n",
        "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
        "            caption,\n",
        "            padding=\"do_not_pad\",\n",
        "            truncation=True,\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "        ).input_ids\n",
        "\n",
        "        if self.class_data_root:\n",
        "            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n",
        "            if class_image.mode != \"RGB\":\n",
        "                class_image = class_image.convert(\"RGB\")\n",
        "            example[\"class_images\"] = self.image_transforms(class_image)\n",
        "            example[\"class_prompt_ids\"] = self.tokenizer(\n",
        "                self.class_prompt,\n",
        "                padding=\"do_not_pad\",\n",
        "                truncation=True,\n",
        "                max_length=self.tokenizer.model_max_length,\n",
        "            ).input_ids\n",
        "\n",
        "        return example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9KUAEDcJyRR"
      },
      "outputs": [],
      "source": [
        "def training_function(text_encoder, vae, unet):\n",
        "    logger = accel_logging.get_logger(__name__)\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    accelerator = Accelerator(\n",
        "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "        mixed_precision=args.mixed_precision,\n",
        "    )\n",
        "\n",
        "    # Freeze VAE parameters; optionally freeze the text encoder as well.\n",
        "    vae.requires_grad_(False)\n",
        "    if not args.train_text_encoder:\n",
        "        text_encoder.requires_grad_(False)\n",
        "\n",
        "    if args.gradient_checkpointing:\n",
        "        unet.enable_gradient_checkpointing()\n",
        "        if args.train_text_encoder:\n",
        "            text_encoder.gradient_checkpointing_enable()\n",
        "\n",
        "    # Use 8-bit Adam if enabled.\n",
        "    optimizer_class = bnb.optim.AdamW8bit if args.use_8bit_adam else torch.optim.AdamW\n",
        "    params_to_optimize = (\n",
        "        itertools.chain(unet.parameters(), text_encoder.parameters())\n",
        "        if args.train_text_encoder else unet.parameters()\n",
        "    )\n",
        "    optimizer = optimizer_class(params_to_optimize, lr=args.learning_rate)\n",
        "\n",
        "    noise_scheduler = DDPMScheduler.from_config(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
        "\n",
        "    # Load the tokenizer (used for prompt encoding)\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
        "\n",
        "    # Create the dataset and dataloader.\n",
        "    # train_dataset = DreamBoothDataset(\n",
        "    #     instance_data_root=args.instance_data_dir,\n",
        "    #     instance_prompt=args.instance_prompt,\n",
        "    #     tokenizer=tokenizer,\n",
        "    #     class_data_root=args.class_data_dir if args.with_prior_preservation else None,\n",
        "    #     class_prompt=args.class_prompt,\n",
        "    #     size=args.resolution,\n",
        "    #     center_crop=args.center_crop,\n",
        "    # )\n",
        "    train_dataset = DreamBoothDataset(\n",
        "        instance_data_root=args.instance_data_dir,\n",
        "        instance_prompts=captions,  # pass your dictionary here\n",
        "        tokenizer=tokenizer,\n",
        "        class_data_root=args.class_data_dir if args.with_prior_preservation else None,\n",
        "        class_prompt=args.class_prompt,\n",
        "        size=args.resolution,\n",
        "        center_crop=args.center_crop,\n",
        "    )\n",
        "\n",
        "\n",
        "    def collate_fn(examples):\n",
        "        input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
        "        pixel_values = [example[\"instance_images\"] for example in examples]\n",
        "        if args.with_prior_preservation:\n",
        "            input_ids += [example[\"class_prompt_ids\"] for example in examples]\n",
        "            pixel_values += [example[\"class_images\"] for example in examples]\n",
        "\n",
        "        pixel_values = torch.stack(pixel_values).to(memory_format=torch.contiguous_format).float()\n",
        "        input_ids = tokenizer.pad(\n",
        "            {\"input_ids\": input_ids},\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=tokenizer.model_max_length\n",
        "        ).input_ids\n",
        "\n",
        "        return {\"input_ids\": input_ids, \"pixel_values\": pixel_values}\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=args.train_batch_size, shuffle=True, collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    lr_scheduler = get_scheduler(\n",
        "        args.lr_scheduler,\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n",
        "        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n",
        "    )\n",
        "\n",
        "    if args.train_text_encoder:\n",
        "        unet, text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "            unet, text_encoder, optimizer, train_dataloader, lr_scheduler\n",
        "        )\n",
        "    else:\n",
        "        unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "            unet, optimizer, train_dataloader, lr_scheduler\n",
        "        )\n",
        "\n",
        "    # Prepare VAE, text encoder for inference (cast to proper dtype)\n",
        "    weight_dtype = torch.float32\n",
        "    if accelerator.mixed_precision == \"fp16\":\n",
        "        weight_dtype = torch.float16\n",
        "    elif accelerator.mixed_precision == \"bf16\":\n",
        "        weight_dtype = torch.bfloat16\n",
        "\n",
        "    vae.to(accelerator.device, dtype=weight_dtype)\n",
        "    vae.decoder.to(\"cpu\")\n",
        "    if not args.train_text_encoder:\n",
        "        text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
        "\n",
        "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
        "    num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
        "\n",
        "    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
        "    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
        "    logger.info(f\"  Total train batch size (w/ accumulation) = {total_batch_size}\")\n",
        "    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
        "\n",
        "    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(num_train_epochs):\n",
        "        unet.train()\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            with accelerator.accumulate(unet):\n",
        "                # Encode images to latent space\n",
        "                latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\n",
        "                latents = latents * 0.18215\n",
        "\n",
        "                noise = torch.randn_like(latents)\n",
        "                bsz = latents.shape[0]\n",
        "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device).long()\n",
        "\n",
        "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "                # Get text embeddings for conditioning\n",
        "                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
        "\n",
        "                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "\n",
        "                # Determine loss target based on prediction type\n",
        "                if noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "                    target = noise\n",
        "                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
        "                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
        "\n",
        "                if args.with_prior_preservation:\n",
        "                    noise_pred, noise_pred_prior = torch.chunk(noise_pred, 2, dim=0)\n",
        "                    target, target_prior = torch.chunk(target, 2, dim=0)\n",
        "                    loss = F.mse_loss(noise_pred.float(), target.float(), reduction=\"none\").mean([1, 2, 3]).mean()\n",
        "                    prior_loss = F.mse_loss(noise_pred_prior.float(), target_prior.float(), reduction=\"mean\")\n",
        "                    loss = loss + args.prior_loss_weight * prior_loss\n",
        "                else:\n",
        "                    loss = F.mse_loss(noise_pred.float(), target.float(), reduction=\"mean\")\n",
        "\n",
        "                accelerator.backward(loss)\n",
        "\n",
        "                if accelerator.sync_gradients:\n",
        "                    accelerator.clip_grad_norm_(unet.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            if accelerator.sync_gradients:\n",
        "                progress_bar.update(1)\n",
        "                global_step += 1\n",
        "\n",
        "                if global_step % args.save_steps == 0:\n",
        "                    if accelerator.is_main_process:\n",
        "                        pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "                            args.pretrained_model_name_or_path,\n",
        "                            unet=accelerator.unwrap_model(unet),\n",
        "                            text_encoder=accelerator.unwrap_model(text_encoder),\n",
        "                        )\n",
        "                        save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n",
        "                        pipeline.save_pretrained(save_path)\n",
        "\n",
        "            progress_bar.set_postfix({\"loss\": loss.detach().item()})\n",
        "            if global_step >= args.max_train_steps:\n",
        "                break\n",
        "        accelerator.wait_for_everyone()\n",
        "        if global_step >= args.max_train_steps:\n",
        "            break\n",
        "\n",
        "    # Save the final model\n",
        "    if accelerator.is_main_process:\n",
        "        pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "            args.pretrained_model_name_or_path,\n",
        "            unet=accelerator.unwrap_model(unet),\n",
        "            text_encoder=accelerator.unwrap_model(text_encoder),\n",
        "        )\n",
        "        pipeline.save_pretrained(args.output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFAuhURfJyRU"
      },
      "outputs": [],
      "source": [
        "# Load base models (text encoder, VAE, and U-Net)\n",
        "text_encoder = CLIPTextModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"text_encoder\")\n",
        "vae = AutoencoderKL.from_pretrained(pretrained_model_name_or_path, subfolder=\"vae\")\n",
        "unet = UNet2DConditionModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"unet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "J4yI3CIyMTQT",
        "outputId": "740d9982-3e4f-43cd-fc1a-2561e736a554",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'nvidia-smi'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-9ccc2de2e80d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Launch training with Accelerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnotebook_launcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/launchers.py\u001b[0m in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes, rdzv_backend, rdzv_endpoint, rdzv_conf, rdzv_id, max_restarts, monitor_interval, log_line_prefix_template)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Launching a training on {device_count()} TPU cores.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mxmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fork\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0min_colab\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mget_gpu_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;31m# No need for a distributed launch otherwise as it's either CPU or one GPU.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/environment.py\u001b[0m in \u001b[0;36mget_gpu_info\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m     \"\"\"\n\u001b[1;32m    122\u001b[0m     \u001b[0;31m# Returns as list of `n` GPUs and their names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     output = subprocess.check_output(\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0m_nvidia_smi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"--query-gpu=count,name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"--format=csv,noheader\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniversal_newlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     )\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0m\u001b[1;32m    467\u001b[0m                **kwargs).stdout\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stderr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1024\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m   1027\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1953\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merr_filename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'nvidia-smi'"
          ]
        }
      ],
      "source": [
        "# Launch training with Accelerate\n",
        "notebook_launcher(training_function, args=(text_encoder, vae, unet))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        },
        "id": "4U_NqQSyJyRV",
        "outputId": "6198a024-2b41-4d48-e9c7-d53ac1b83e98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Couldn't connect to the Hub: 401 Client Error. (Request ID: Root=1-67dd8261-03a49989048fcc821dbeed38;bdc6e1e5-2dd8-49fe-8f92-18b5ea2b22d6)\n",
            "\n",
            "Repository Not Found for url: https://huggingface.co/api/models/dreambooth-bruno.\n",
            "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
            "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
            "Invalid username or password..\n",
            "Will try to load from local cache.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Cannot load model dreambooth-bruno: model is not cached locally and an error occurred while trying to fetch metadata from the Hub. Please check out the root cause in the stacktrace above.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models/dreambooth-bruno",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/pipelines/pipeline_utils.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(cls, pretrained_model_name, **kwargs)\u001b[0m\n\u001b[1;32m   1399\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1400\u001b[0;31m                 \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1401\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOfflineModeIsEnabled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConnectionError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mmodel_info\u001b[0;34m(self, repo_id, revision, timeout, securityStatus, files_metadata, expand, token)\u001b[0m\n\u001b[1;32m   2518\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2519\u001b[0;31m         \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2520\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    457\u001b[0m             )\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-67dd8261-03a49989048fcc821dbeed38;bdc6e1e5-2dd8-49fe-8f92-18b5ea2b22d6)\n\nRepository Not Found for url: https://huggingface.co/api/models/dreambooth-bruno.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-71306d07bbb4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m pipe = StableDiffusionPipeline.from_pretrained(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# your fine-tuned model directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/pipelines/pipeline_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    768\u001b[0m                     \u001b[0;34m\" is neither a valid local path nor a valid repo id. Please check the parameter.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m                 )\n\u001b[0;32m--> 770\u001b[0;31m             cached_folder = cls.download(\n\u001b[0m\u001b[1;32m    771\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m                 \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/pipelines/pipeline_utils.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(cls, pretrained_model_name, **kwargs)\u001b[0m\n\u001b[1;32m   1592\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m                 \u001b[0;31m# 2. we forced `local_files_only=True` when `model_info` failed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1594\u001b[0;31m                 raise EnvironmentError(\n\u001b[0m\u001b[1;32m   1595\u001b[0m                     \u001b[0;34mf\"Cannot load model {pretrained_model_name}: model is not cached locally and an error occurred\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1596\u001b[0m                     \u001b[0;34m\" while trying to fetch metadata from the Hub. Please check out the root cause in the stacktrace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Cannot load model dreambooth-bruno: model is not cached locally and an error occurred while trying to fetch metadata from the Hub. Please check out the root cause in the stacktrace above."
          ]
        }
      ],
      "source": [
        "from diffusers import DPMSolverMultistepScheduler\n",
        "\n",
        "scheduler = DPMSolverMultistepScheduler.from_pretrained(\n",
        "    pretrained_model_name_or_path,  # original model path with scheduler config\n",
        "    subfolder=\"scheduler\"\n",
        ")\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    args.output_dir,  # your fine-tuned model directory\n",
        "    scheduler=scheduler,\n",
        "    torch_dtype=torch.float16,\n",
        ").to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "283cMoj_JyRX"
      },
      "outputs": [],
      "source": [
        "def inference(prompt, num_samples=1, num_inference_steps=100, guidance_scale=20):\n",
        "    images = pipe(prompt, num_images_per_prompt=num_samples, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale).images\n",
        "    return images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1PwKugqMOZe"
      },
      "outputs": [],
      "source": [
        "# Test with your custom token\n",
        "prompt = \"a photo of <bruno> on the moon\"\n",
        "generated_images = inference(prompt, num_samples=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_b8TT_IMQv5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Example: Display multiple images in a loop\n",
        "for img in generated_images:\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")  # Hide axes\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DPj9lwpUrQD"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies if needed\n",
        "!pip install transformers torch torchvision diffusers accelerate\n",
        "!pip install scikit-image\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "from skimage.metrics import structural_similarity\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load CLIP model for evaluation\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Preprocessing for CLIP model\n",
        "clip_preprocess = Compose([\n",
        "    Resize((224, 224)),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "              std=[0.26862954, 0.26130258, 0.27577711])\n",
        "])\n",
        "\n",
        "# Function to calculate CLIP score\n",
        "def calculate_clip_score(image, prompt):\n",
        "    inputs = clip_processor(text=[prompt], images=image, return_tensors=\"pt\", padding=True).to(device)\n",
        "    outputs = clip_model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image\n",
        "    probs = logits_per_image.softmax(dim=1)\n",
        "    return probs[0][0].item() * 100  # Convert to percentage\n",
        "\n",
        "# Function to calculate SSIM score\n",
        "def calculate_ssim(real_image, generated_image):\n",
        "    real_np = np.array(real_image.resize((512, 512))).astype(np.float32) / 255.0\n",
        "    gen_np = np.array(generated_image.resize((512, 512))).astype(np.float32) / 255.0\n",
        "    ssim_score = structural_similarity(real_np, gen_np, channel_axis=-1, data_range=1.0)\n",
        "    return ssim_score * 100  # percentage for clarity\n",
        "\n",
        "# Main Evaluation function\n",
        "def evaluate_model(prompts, real_images_paths, pipe):\n",
        "    clip_scores = []\n",
        "    ssim_scores = []\n",
        "\n",
        "    for idx, prompt in enumerate(prompts):\n",
        "        # Generate image from prompt\n",
        "        generated_image = pipe(prompt=prompt, num_inference_steps=100, guidance_scale=20).images[0]\n",
        "\n",
        "        # Load corresponding real image\n",
        "        real_image = Image.open(real_images_paths[idx]).convert(\"RGB\")\n",
        "\n",
        "        # Calculate CLIP score\n",
        "        clip_score = calculate_clip_score(generated_image, prompt)\n",
        "        clip_scores.append(clip_score)\n",
        "\n",
        "        # Calculate SSIM score\n",
        "        ssim_score = calculate_ssim(real_image, generated_image)\n",
        "        ssim_scores.append(ssim_score)\n",
        "\n",
        "        # Display scores\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "        print(f\"CLIP Score: {clip_score:.2f}%\")\n",
        "        print(f\"SSIM Score: {ssim_score:.2f}%\\n\")\n",
        "\n",
        "        # Optional: Display generated vs real image\n",
        "        print(\"Generated vs Real Image:\")\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "        axes[0].imshow(generated_image)\n",
        "        axes[0].set_title(\"Generated Image\")\n",
        "        axes[0].axis(\"off\")\n",
        "        axes[1].imshow(real_image)\n",
        "        axes[1].set_title(\"Real Image\")\n",
        "        axes[1].axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "    # Average scores\n",
        "    print(f\"\\nAverage CLIP Score: {np.mean(clip_scores):.2f}%\")\n",
        "    print(f\"Average SSIM Score: {np.mean(ssim_scores):.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "fn_dqXjsNg1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_prompts = [\n",
        "    \"a photo of <bruno> at the beach\",\n",
        "    \"a photo of <bruno> in a superhero costume\"\n",
        "]\n",
        "\n",
        "real_images_paths = [\n",
        "    \"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan/IMG_20230423_082747.jpg\",\n",
        "    \"/content/drive/MyDrive/CS614 Final Project/Dataset/Milan/764c4646-791c-4ebb-9686-6a368573e673.jpg\"\n",
        "]\n",
        "\n",
        "# Call the evaluation function\n",
        "evaluate_model(test_prompts, real_images_paths, pipe)\n"
      ],
      "metadata": {
        "id": "amYVNnn197SE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}